# BuildYourGPT

This repository contains a simple implementation of a Generative Pretrained Transformer (GPT). The project primarily follows the research paper **"Attention Is All You Need"**, with additional insights from **"Deep Residual Learning for Image Recognition"**, **"Dropout: A Simple Way to Prevent Neural Networks from Overfitting"**,
**"Deep Residual Learning for Image Recognition"**, and a few other sources.

## About

This a **learning project** where I followed Andrej Karpathy's tutorial to build a basic transformer model.

The core idea comes from the **"Attention Is All You Need"** paper by Google engineers, which introduced the **Transformer architecture**â€”the foundation of modern models like GPT and ChatGPT.

## References

- **Andrej Karpathy's tutorial:**  
  [https://youtu.be/kCc8FmEb1nY?si=SUSakqEUBg8tCgZ1](https://youtu.be/kCc8FmEb1nY?si=C_hW8fNZ1Lft_yGc)  

- **"Attention Is All You Need" Paper:**  
  [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)  

- **"Deep Residual Learning for Image Recognition" Paper:**  
  [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)  

- **"Dropout: A Simple Way to Prevent Neural Networks from Overfitting" Paper:**  
  [https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)  

## Features

- Transformer-based language model  
- Implements **self-attention** and **positional encoding**  
- Tokenization and text preprocessing  
- Simple training loop for text generation  

## Disclaimer

This project is for **learning purposes only**. The main credit goes to **Andrej Karpathy** and the researchers behind **Transformers**.
